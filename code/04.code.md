# Building Advanced AI Assistants: The Toolkit for the Augmented Developer


## OpenAI Chat API


```bash
OPENAI_API_KEY=sk-...
```


```bash
pip install openai==1.23.6 python-dotenv==1.0.1
```


```python
from openai import OpenAI
from dotenv import dotenv_values

# We load the OpenAI API key from the environment
config = dotenv_values(".env")
openai_api_key = config["OPENAI_API_KEY"]
# We initialize the OpenAI client
client = OpenAI(
    api_key=openai_api_key,
)
```


```python
# We define the model to use
model = "gpt-4"
# We define the question to ask the assistant
question = "Count the number of files in the current directory."
# We define the messages to send to the assistant
messages = [
  {
    "role": "system",
    "content": 
      "You are a smart assistant."
  },
  {
    "role": "user",
    "content": 
      "List all the files in the current "
      "directory."
  },
  {
    "role": "assistant",
    "content": 
      "ls -l"
  },
  {
    "role": "user",
    "content":       
      "List all the files in the current "
      "directory, including hidden files."
  },
  {
    "role": "assistant",
    "content": 
      "ls -la"
  },
  {
    "role": "user",
    "content": 
      "Delete all the files in the current "
      "directory."
  },
  {
    "role": "assistant",
    "content": 
      "rm *"
  },
  {
    "role": "user",
    "content": 
      "Count the number of occurrences of "
      "the word 'sun' in the file 'test.txt'."
  },
  {
    "role": "assistant",
    "content": 
      "grep -o 'sun' test.txt | wc -l"
  },
  {
    "role": "user",
    "content": question      
  }
]
```


```python
response = client.chat.completions.create(
    model=model,
    messages=messages,
    max_tokens=200,
    temperature=0,
)

output = response.choices[0].message.content.strip()
print(output)
```


```bash
ls -l | wc -l
```


```bash
pip install click==8.1.3
```


```python
from openai import OpenAI
from dotenv import dotenv_values
import click

# We load the OpenAI API key from the environment
config = dotenv_values(".env")
openai_api_key = config["OPENAI_API_KEY"]
# We initialize the OpenAI client
client = OpenAI(
    api_key=openai_api_key,
)
# We define the model to use
model = "gpt-4"
# We define the question to ask the assistant
question = "Count the number of files in the current directory."
# We define the messages to send to the assistant
base_messages = [
  {
    "role": "system",
    "content": 
      "You are a smart assistant."
  },
  {
    "role": "user",
    "content": 
      "List all the files in the current "
      "directory."
  },
  {
    "role": "assistant",
    "content": 
      "ls -l"
  },
  {
    "role": "user",
    "content":       
      "List all the files in the current "
      "directory, including hidden files."
  },
  {
    "role": "assistant",
    "content": 
      "ls -la"
  },
  {
    "role": "user",
    "content": 
      "Delete all the files in the current "
      "directory."
  },
  {
    "role": "assistant",
    "content": 
      "rm *"
  },
  {
    "role": "user",
    "content": 
      "Count the number of occurrences of "
      "the word 'sun' in the file 'test.txt'."
  },
  {
    "role": "assistant",
    "content": 
      "grep -o 'sun' test.txt | wc -l"
  },
  {
    "role": "user",
    "content": question      
  }
]

# Infinite loop to continuously interact with the user
while True:
    # Copy the base messages to start a fresh conversation context
    messages = base_messages.copy()

    # Read the user input, styling the prompt text in green
    request = input(
      click.style(
        "Input: ", 
        fg="green"
      )
    )
    
    # Add the user input to the list of messages with a user role identifier
    messages.append(
        {
            "role": "user",
            "content": f"{request}"
        }
    )

    # Send the accumulated messages to the OpenAI API to get a response
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=200,
        temperature=0,        
    )
    
    # Extract the command from the API's response, removing any extra whitespace
    command = response.choices[0].message.content.strip()    

    # Print the API's response, styling the prefix text in yellow
    click.echo(
      click.style(
        "Output: ", fg="yellow"
      ) + command
    )
    # Print a blank line for better readability between interactions
    click.echo()
```


```bash
python assistant.py
```


```yaml
Input: Show the hidden files in the current directory
Output: ls -a

Input: Show the highest memory-consuming 5 processes
Output: ps aux --sort=-%mem | head -n 6

Input: Append the content of file1 to file2
Output: cat file1 >> file2
```


```python
# assistant.py
# ...
while True:
    # ...
    if request.lower() in ["exit", "quit"]:
        break    
    # ...
```


```python
from openai import OpenAI
from dotenv import dotenv_values
import click, os

# We load the OpenAI API key from the environment
config = dotenv_values(".env")
openai_api_key = config["OPENAI_API_KEY"]
# We initialize the OpenAI client
client = OpenAI(
    api_key=openai_api_key,
)
# We define the model to use
model = "gpt-4"
# We define the question to ask the assistant
question = "Count the number of files in the current directory."
# We define the messages to send to the assistant
base_messages = [
  {
    "role": "system",
    "content": 
      "You are a smart assistant."
  },
  {
    "role": "user",
    "content": 
      "List all the files in the current "
      "directory."
  },
  {
    "role": "assistant",
    "content": 
      "ls -l"
  },
  {
    "role": "user",
    "content":       
      "List all the files in the current "
      "directory, including hidden files."
  },
  {
    "role": "assistant",
    "content": 
      "ls -la"
  },
  {
    "role": "user",
    "content": 
      "Delete all the files in the current "
      "directory."
  },
  {
    "role": "assistant",
    "content": 
      "rm *"
  },
  {
    "role": "user",
    "content": 
      "Count the number of occurrences of "
      "the word 'sun' in the file 'test.txt'."
  },
  {
    "role": "assistant",
    "content": 
      "grep -o 'sun' test.txt | wc -l"
  },
  {
    "role": "user",
    "content": question      
  }
]

while True:
    messages = base_messages.copy()

    # read the user input
    request = input(
      click.style(
        "Input  (type 'exit' to quit): ", 
        fg="green"
      )
    )
    
    if request.lower() in ["exit", "quit"]:
        break    

    # add the user input to the messages
    messages.append(
        {
            "role": "user",
            "content": f"{request}"
        }
    )

    # send the messages to the API
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=200,
        temperature=0,        
    )
    
    # get the command from the response
    command = response.choices[0].message.content.strip()    

    # Print the command in a nice way
    click.echo(
      click.style(
        "Output: ", 
        fg="yellow"
      ) + command
    )
    
    # Ask the user if they want to execute the command
    click.echo(
      click.style(
        "Execute? (y/n): ", 
        fg="yellow"), 
        nl=False
    )
    # Read the user input
    choice = input()
    
    # If the user wants to execute the command, execute it
    if choice == "y":
      os.system(command)
    elif choice == "n":
      continue
    else:
      click.echo(
        click.style(
          "Invalid choice. Please enter 'y' or 'n'.", 
          fg="red"
        )
      )
    
    click.echo()
```


```text
Input (type 'exit' to quit): Create a new file named 'test.txt' in /tmp
Output: touch /tmp/test.txt
Execute? (y/n): y
<no output here>

Input (type 'exit' to quit): list all files in /tmp
Output: ls /tmp
Execute? (y/n): y
<output of the command ls /tmp>

Input (type 'exit' to quit): Show the content of the file '/tmp/test.txt'
Output: cat /tmp/test.txt
Execute? (y/n): cool
Invalid choice. Please enter 'y' or 'n'.
```


```python
from openai import OpenAI
from dotenv import dotenv_values
import click, os

# We load the OpenAI API key from the environment
config = dotenv_values(".env")
openai_api_key = config["OPENAI_API_KEY"]
# We initialize the OpenAI client
client = OpenAI(
    api_key=openai_api_key,
)
# We define the model to use
model = "gpt-4"
# We define the question to ask the assistant
question = "Count the number of files in the current directory."
# We define the messages to send to the assistant
base_messages = [
  {
    "role": "system",
    "content": 
      "You are a smart assistant."
  },
  {
    "role": "user",
    "content": 
      "List all the files in the current "
      "directory."
  },
  {
    "role": "assistant",
    "content": 
      "ls -l"
  },
  {
    "role": "user",
    "content":       
      "List all the files in the current "
      "directory, including hidden files."
  },
  {
    "role": "assistant",
    "content": 
      "ls -la"
  },
  {
    "role": "user",
    "content": 
      "Delete all the files in the current "
      "directory."
  },
  {
    "role": "assistant",
    "content": 
      "rm *"
  },
  {
    "role": "user",
    "content": 
      "Count the number of occurrences of "
      "the word 'sun' in the file 'test.txt'."
  },
  {
    "role": "assistant",
    "content": 
      "grep -o 'sun' test.txt | wc -l"
  },
  {
    "role": "user",
    "content": question      
  }
]

while True:
    messages = base_messages.copy()

    # read the user input
    request = input(
      click.style(
        "Input: (type 'exit' to quit): ", 
        fg="green"
      )
    )
    
    if request.lower() in ["exit", "quit"]:
        break    

    # add the user input to the messages
    messages.append(
        {
            "role": "user",
            "content": f"{request}"
        }
    )

    # send the messages to the API
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=200,
        temperature=0,        
    )
    
    # get the command from the response
    command = response.choices[0].message.content.strip()    

    # Print the command in a nice way
    click.echo(
      click.style(
        "Output: ", 
        fg="yellow"
      ) + command
    )
    
    # Ask the user if they want to execute the command
    click.echo(
      click.style(
        "Execute? (y/n): ", 
        fg="yellow"
      ), 
      nl=False
    )
    # Read the user input
    choice = input()
    
    # If the user wants to execute the command, execute it
    if choice == "y":
      r = os.system(command)
      if r != 0:
        click.echo(
          click.style(
            "Error executing command.", 
            fg="red"
          )
        )
    # If the user doesn't want to execute the command, continue    
    elif choice == "n":
      continue
    else:
      click.echo(
        click.style(
          "Invalid choice. Please enter 'y' or 'n'.", 
          fg="red"
        )
      )
    
    click.echo()
```


```yaml
Input: (type 'exit' to quit): Create a file /tmp/random.log
Output: touch /tmp/random.log
Execute? (y/n): y

Input: (type 'exit' to quit): remove file /tmp/unknown.log
Output: rm /tmp/unknown.log
Execute? (y/n): y
rm: cannot remove '/tmp/unknown.log': No such file or directory
Error executing command.
```


## OpenAI Assistants API


```python
from openai import OpenAI
from dotenv import dotenv_values

# We load the OpenAI API key from the environment
config = dotenv_values(".env")
openai_api_key = config["OPENAI_API_KEY"]
# We initialize the OpenAI client
client = OpenAI(
    api_key=openai_api_key,
)
```


```python
# We create the assistant
assistant_model = "gpt-4"
assistant_name = "DevBot"
assistant_instructions = f"You are {assistant_name}, a smart developer assistant."
tools = [{"type": "code_interpreter"}]

assistant = client.beta.assistants.create(
    model=assistant_model,
    instructions=assistant_instructions,
    name=assistant_name,
    tools=tools
)

# Create a Thread
thread = client.beta.threads.create()
```


```python
# Ask the user for their name
print(f"{assistant_name}: Hello there! I am your developer assistant.\nWhat is your name?")
user_name = input("You: ")

# Ask the user for a question
print(f"{assistant_name}: Hi {user_name}! Ask me a question.")
message_content = input(f"{user_name}: ")
```


```python
# Add the user's question to the thread
message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content=message_content
)
```


```python
# Define the run instructions
run_instructions = f"""
You are talking with {user_name}. 
You can call them by their name.
Please reply to their question. 
Share code snippets and examples when necessary.
"""

# Create a run with the assistant
run = client.beta.threads.runs.create(
    thread_id=thread.id,
    assistant_id=assistant.id,
    instructions=run_instructions
)
```


```python
# Monitor the status of the run
# Wait for the assistant to complete the run
while True:
	status = client.beta.threads.runs.retrieve(
        thread_id=thread.id,
        run_id=run.id
    ).status

	if status == "completed":
		messages = client.beta.threads.messages.list(
            thread_id=thread.id
		)
		print(f"{assistant_name}: {messages.data[0].content[0].text.value}")
		break
```


```python
from openai import OpenAI
from dotenv import dotenv_values

# We load the OpenAI API key from the environment
config = dotenv_values(".env")
openai_api_key = config["OPENAI_API_KEY"]
# We initialize the OpenAI client
client = OpenAI(
    api_key=openai_api_key,
)

# We create the assistant
assistant_model = "gpt-4"
assistant_name = "DevBot"
assistant_instructions = f"You are {assistant_name}, a smart developer assistant."
tools = [{"type": "code_interpreter"}]

assistant = client.beta.assistants.create(
    model=assistant_model,
    instructions=assistant_instructions,
    name=assistant_name,
    tools=tools
)

# Create a Thread
thread = client.beta.threads.create()

# Ask the user for their name
print(f"{assistant_name}: Hello there! I am your developer assistant.\nWhat is your name?")
user_name = input("You: ")

# Ask the user for a question
print(f"{assistant_name}: Hi {user_name}! Ask me a question.")
message_content = input(f"{user_name}: ")

# Add the user's question to the thread
message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content=message_content
)

# Define the run instructions
run_instructions = f"""
You are talking with {user_name}. 
You can call them by their name.
Please reply to their question. 
Share code snippets and examples when necessary.
"""

# Create a run with the assistant
run = client.beta.threads.runs.create(
    thread_id=thread.id,
    assistant_id=assistant.id,
    instructions=run_instructions
)

# Monitor the status of the run
# Wait for the assistant to complete the run
while True:
	status = client.beta.threads.runs.retrieve(
        thread_id=thread.id,
        run_id=run.id
    ).status

	if status == "completed":		
		messages = client.beta.threads.messages.list(
            thread_id=thread.id
		)
		print(f"{assistant_name}: {messages.data[0].content[0].text.value}")
		break
```


```bash
python assistant.py
```


```yaml
DevBot: Hello there! I am your developer assistant.
What is your name?
You: John
DevBot: Hi John! Ask me a question.
John: What's the result of (1234 * 5678)/2?
DevBot: ...
```


```python
from openai import OpenAI
from dotenv import dotenv_values
[...]
[...]
[...]
# Ask the user for a question
print(f"{assistant_name}: Hi {user_name}! Ask me a question.")

while True:
    message_content = input(f"{user_name}: (Type 'exit' to quit) ")
    if message_content == "exit":
        break

    # Add the user's question to the thread
    message = client.beta.threads.messages.create(
        thread_id=thread.id,
        role="user",
        content=message_content
    )

    # Define the run instructions
    run_instructions = f"""
    You are talking with {user_name}. 
    You can call them by their name.
    Please reply to their question. 
    Share code snippets and examples when necessary.
    """

    # Create a run with the assistant
    run = client.beta.threads.runs.create(
        thread_id=thread.id,
        assistant_id=assistant.id,
        instructions=run_instructions
    )

    # Monitor the status of the run
    # Wait for the assistant to complete the run
    while True:
        status = client.beta.threads.runs.retrieve(
            thread_id=thread.id,
            run_id=run.id
        ).status

        if status == "completed":
            messages = client.beta.threads.messages.list(
                thread_id=thread.id
            )
            print(
                f"{assistant_name}: {messages.data[0].content[0].text.value}")
            break
```


## LangChain, RAG, Chroma, and Embeddings


```bash
pip install langchain==0.1.16 langchain-openai==0.1.3
```


```bash
export OPENAI_API_KEY="sk-..."
```


```python
from langchain_openai import ChatOpenAI
import os

# Get the OpenAI API key from the environment
openai_api_key = os.getenv("OPENAI_API_KEY")

# This can be gpt-4 or any other model
model = "gpt-3.5-turbo"
# max_tokens is the maximum number of tokens to generate
max_tokens = 300
# Temperature controls the randomness of the generated text
temperature = 1

# Create a ChatOpenAI instance
llm = ChatOpenAI(
    model=model,
    max_tokens=max_tokens,
    temperature=temperature
)

# Invoke the model to generate a response
response = llm.invoke("How to code like a hero?")

# Print the generated response
print(response.content)
```


```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import os

# Get the OpenAI API key from the environment
openai_api_key = os.getenv("OPENAI_API_KEY")

# This can be gpt-4 or any other model
model = "gpt-3.5-turbo"
# max_tokens is the maximum number of tokens to generate
max_tokens = 300
# Temperature is a hyperparameter that controls the randomness of the generated text
temperature = 1

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a smart developer who help other developers to reach financial independence."),
    ("user", "{input}")
])

# Create a ChatOpenAI instance
llm = ChatOpenAI(
    model=model,
    max_tokens=max_tokens,
    temperature=temperature
)

chain = prompt | llm

# Run the chain
output = chain.invoke("What's the best way to reach the financial independence as a developer?")

print(output.content)
```


```text
What is the capital of France?
```


```text
1. Paris is the capital of France.
2. The Eiffel Tower is located in Paris.
3. The Louvre Museum is located in Paris.
4. The Seine River flows through Paris.
5. The French Revolution started in Paris.
```


```python
[0.1, 0.2, 0.3, 0.4, 0.6]
```


```python
{
    "1": [0.2, 0.3, 0.4, 0.5, 0.6],
    "2": [0.3, 0.4, 0.5, 0.6, 0.7],
    "3": [0.4, 0.5, 0.6, 0.7, 0.8],
    "4": [0.5, 0.6, 0.7, 0.8, 0.9],
    "5": [0.6, 0.7, 0.8, 0.9, 1.0]
}
```


## Good Documentation Can Enhance Developer Productivity, but a Smart Assistant Can Do More


```bash
pip install \
requests==2.31.0 \
pysqlite3==0.5.2 \
pysqlite3==0.5.2 \
bs4==0.0.2 \
python-dotenv==1.0.1 \
langchain-community==0.0.34 \
langchain==0.1.16 \
langchain-openai==0.1.4 \
unstructured==0.11.8 \
chromadb==0.5.0 \
pysqlite3-binary==0.5.2.post3
```


```python
# If you are on a Linux system, 
# you can install pysqlite3-binary, pip install pysqlite3-binary 
# and then override the default sqlite3 library before running Chroma
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import os, requests, uuid
from bs4 import BeautifulSoup
from dotenv import dotenv_values
from langchain_community.document_loaders import DirectoryLoader
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
```


```bash
OPENAI_API_KEY=sk-...
```


```python
# We load the OpenAI API key from the environment
config = dotenv_values(".env")
openai_api_key = config["OPENAI_API_KEY"]
```


```python
def save_webpage_content(
        url, 
        random_directory
    ):    
    """
    This function saves the content of a webpage to a file    
    """
    response = requests.get(url, timeout=5)
    response.raise_for_status()
    soup = BeautifulSoup(response.content, "html.parser")

    text = soup.get_text()

    if not os.path.exists(random_directory):
        os.makedirs(random_directory)    
    
    file_name = str(uuid.uuid4()) + ".html"
    file_path = os.path.join(random_directory, file_name)
    
    with open(file_path, "w") as f:
        f.write(text)        

def create_loader(
        directory, 
        file_extension=".html", 
        recursive=True
    ):
    """
    Load documents from a directory using 
    the Langchain DirectoryLoader class
    The loader will create a document for each file
    """
    loader = DirectoryLoader(
        directory,
        glob="**/*" + file_extension,
        recursive=recursive
    )

    docs = loader.load()
    return docs

def split_docs(
        documents,
        chunk_size=1000,
        chunk_overlap=20
    ):
    """
    Split documents into smaller chunks
    to avoid context size limitations
    when sending the documents to OpenAI
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, 
        chunk_overlap=chunk_overlap
    )
    docs = text_splitter.split_documents(documents)
    return docs

def create_embedding_function(
        api_key, 
        show_progress_bar=False
    ):
    """
    Create an embedding function using the OpenAI API
    """    
    return OpenAIEmbeddings(
        api_key=api_key,
        show_progress_bar=show_progress_bar,
    )

def create_db(
        docs, 
        embedding_function, 
        persist_directory, 
        collection_name
    ):
    """
    Create a database from the documents
    and the embedding function
    """
    db = Chroma.from_documents(
        docs, 
        embedding_function,
        persist_directory=persist_directory,  
        collection_name=collection_name,          
    )
    db.persist()
    return db


def create_prompt(template):
    """
    Create a prompt from a template
    """
    return ChatPromptTemplate.from_template(template)
```


```bash
python assistant.py "<question>" "<url-1>" "<url-2>" ... "<url-n>"
```


```python
# The question we want to ask will be passed 
# as an argument by the user
question = sys.argv[1]
# We get the URLs from the command line arguments
urls = sys.argv[2:]
```


```python
# Define a random unique file to save the webpages
random_directory = "data/" + str(uuid.uuid4())
# We save the webpages to the files in the data directory
for url in urls:
    save_webpage_content(url, random_directory)
```


```python
# We load the documents from the directory    
docs = create_loader(random_directory)
```


```python
# We split documents into smaller chunks to make the retrieval faster
# Note that models like OpenAI does have a limit on the context size
# Therefore, splitting the documents into smaller chunks can be useful
splitted_docs = split_docs(docs)
```


```python
# We embed the split documents
embedding_function = create_embedding_function(openai_api_key)
```


```python
# We create a database containing the splitted documents
# and their embeddings
db = create_db(
        splitted_docs, 
        embedding_function, 
        os.path.join("data", "chroma"),
        "webpages"
    )
```


```python
# We create a retriever from the database.
# A retriever involves creating embeddings 
# for each piece of text.
retriever = db.as_retriever()
```


```python
# We create a prompt template that contains the context and the question
template = """Answer the question based only on the following context.
Provide code examples when possible.
If you don't know the answer, you can appologize 
and tell the user that the question is out of your expertise.

The context is the following text: {context}

The user question is: {question}
"""

# We create a prompt from the template
prompt = create_prompt(template)
```


```python
# We create an instance of the OpenAI model
model = ChatOpenAI(api_key=openai_api_key)
```


```python
# We create a chain that first retrieves the context
# and then passes it to the prompt
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)
```


```python
# We run the chain by invoking it with the input of the user
response = chain.invoke(question)

# We print the response
print(response)
```


```python
# If you are on a Linux system, 
# you can install pysqlite3-binary, pip install pysqlite3-binary 
# and then override the default sqlite3 library before running Chroma
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import os, requests, uuid
from bs4 import BeautifulSoup
from dotenv import dotenv_values
from langchain_community.document_loaders import DirectoryLoader
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter


# We load the OpenAI API key from the environment
config = dotenv_values(".env")
openai_api_key = config["OPENAI_API_KEY"]

def save_webpage_content(
        url, 
        random_directory
    ):    
    """
    This function saves the content of a webpage to a file    
    """
    response = requests.get(url, timeout=5)
    response.raise_for_status()
    soup = BeautifulSoup(response.content, "html.parser")

    text = soup.get_text()

    if not os.path.exists(random_directory):
        os.makedirs(random_directory)    
    
    file_name = str(uuid.uuid4()) + ".html"
    file_path = os.path.join(random_directory, file_name)
    
    with open(file_path, "w") as f:
        f.write(text)        

def create_loader(
        directory, 
        file_extension=".html", 
        recursive=True
    ):
    """
    Load documents from a directory using 
    the Langchain DirectoryLoader class
    The loader will create a document for each file
    """
    loader = DirectoryLoader(
        directory,
        glob="**/*" + file_extension,
        recursive=recursive
    )

    docs = loader.load()
    return docs

def split_docs(
        documents,
        chunk_size=1000,
        chunk_overlap=20
    ):
    """
    Split documents into smaller chunks
    to avoid context size limitations
    when sending the documents to OpenAI
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, 
        chunk_overlap=chunk_overlap
    )
    docs = text_splitter.split_documents(documents)
    return docs

def create_embedding_function(
        api_key, 
        show_progress_bar=False
    ):
    """
    Create an embedding function using the OpenAI API
    """    
    return OpenAIEmbeddings(
        api_key=api_key,
        show_progress_bar=show_progress_bar,
    )

def create_db(
        docs, 
        embedding_function, 
        persist_directory, 
        collection_name
    ):
    """
    Create a database from the documents
    and the embedding function
    """
    db = Chroma.from_documents(
        docs, 
        embedding_function,
        persist_directory=persist_directory,  
        collection_name=collection_name,          
    )
    db.persist()
    return db


def create_prompt(template):
    """
    Create a prompt from a template
    """
    return ChatPromptTemplate.from_template(template)

# The question we want to ask will be passed 
# as an argument by the user
question = sys.argv[1]
# We get the URLs from the command line arguments
urls = sys.argv[2:]

# Define a random unique file to save the webpages
random_directory = "data/" + str(uuid.uuid4())
# We save the webpages to the files in the data directory
for url in urls:
    save_webpage_content(url, random_directory)
    

# We load the documents from the directory    
docs = create_loader(random_directory)

# We split documents into smaller chunks to make the retrieval faster
# Note that models like OpenAI does have a limit on the context size
# Therefore, splitting the documents into smaller chunks can be useful
splitted_docs = split_docs(docs)

# We embed the splitted documents
embedding_function = create_embedding_function(openai_api_key)

# We create a database containing the splitted documents
# and their embeddings
db = create_db(
        splitted_docs, 
        embedding_function, 
        os.path.join("data", "chroma"),
        "webpages"
    )

# We create a retriever from the database.
# A retriever involves creating embeddings 
# for each piece of text.
retriever = db.as_retriever()

# We create a prompt template that contains the context and the question
template = """Answer the question based only on the following context.
Provide code examples when possible.
If you don't know the answer, you can appologize 
and tell the user that the question is out of your expertise.

The context is the following text: {context}

The user question is: {question}
"""

# We create a prompt from the template
prompt = create_prompt(template)

# We create an instance of the OpenAI model
model = ChatOpenAI(api_key=openai_api_key,)

# We create a chain that first retrieves the context
# and then passes it to the prompt
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

# We run the chain by invoking it with the input of the user
response = chain.invoke(question)

# We print the response
print(response)
```


```bash
python app.py \
    "How to create a custom authentication?" \
    https://docs.djangoproject.com/en/5.0/topics/auth/ \
    https://testdriven.io/blog/django-custom-user-model/
```


```python
# If you are on a Linux system, 
# you can install pysqlite3-binary, pip install pysqlite3-binary 
# and then override the default sqlite3 library before running Chroma
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import os, requests, uuid
from bs4 import BeautifulSoup
from dotenv import dotenv_values
from langchain_community.document_loaders import DirectoryLoader
from langchain_community.vectorstores import Chroma
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain


# We load the OpenAI API key from the environment
config = dotenv_values(".env")
openai_api_key = config["OPENAI_API_KEY"]

def save_webpage_content(
        url, 
        random_directory
    ):    
    """
    This function saves the content of a webpage to a file    
    """
    response = requests.get(url, timeout=5)
    response.raise_for_status()
    soup = BeautifulSoup(response.content, "html.parser")

    text = soup.get_text()

    if not os.path.exists(random_directory):
        os.makedirs(random_directory)    
    
    file_name = str(uuid.uuid4()) + ".html"
    file_path = os.path.join(random_directory, file_name)
    
    with open(file_path, "w") as f:
        f.write(text)        

def create_loader(
        directory, 
        file_extension=".html", 
        recursive=True
    ):
    """
    Load documents from a directory using 
    the Langchain DirectoryLoader class
    The loader will create a document for each file
    """
    loader = DirectoryLoader(
        directory,
        glob="**/*" + file_extension,
        recursive=recursive
    )

    docs = loader.load()
    return docs

def split_docs(
        documents,
        chunk_size=1000,
        chunk_overlap=20
    ):
    """
    Split documents into smaller chunks
    to avoid context size limitations
    when sending the documents to OpenAI
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, 
        chunk_overlap=chunk_overlap
    )
    docs = text_splitter.split_documents(documents)
    return docs

def create_embedding_function(
        api_key, 
        show_progress_bar=False
    ):
    """
    Create an embedding function using the OpenAI API
    """    
    return OpenAIEmbeddings(
        api_key=api_key,
        show_progress_bar=show_progress_bar,
    )

def create_db(
        docs, 
        embedding_function, 
        persist_directory, 
        collection_name
    ):
    """
    Create a database from the documents
    and the embedding function
    """
    db = Chroma.from_documents(
        docs, 
        embedding_function,
        persist_directory=persist_directory,  
        collection_name=collection_name,          
    )
    db.persist()
    return db

# The question we want to ask will be passed 
# as an argument by the user
question = sys.argv[1]
# We get the URLs from the command line arguments
urls = sys.argv[1:]

# Define a random unique file to save the webpages
random_directory = "data/" + str(uuid.uuid4())
# We save the webpages to the files in the data directory
for url in urls:
    save_webpage_content(url, random_directory)
    

# We load the documents from the directory    
docs = create_loader(random_directory)

# We split documents into smaller chunks to make the retrieval faster
# Note that models like OpenAI does have a limit on the context size
# Therefore, splitting the documents into smaller chunks can be useful
splitted_docs = split_docs(docs)

# We embed the splitted documents
embedding_function = create_embedding_function(openai_api_key)

# We create a database containing the splitted documents
# and their embeddings
db = create_db(
        splitted_docs, 
        embedding_function, 
        os.path.join("data", "chroma"),
        "webpages"
    )

# We create a retriever from the database.
# A retriever involves creating embeddings 
# for each piece of text.
retriever = db.as_retriever()

# We create an instance of the OpenAI model
model = ChatOpenAI(api_key=openai_api_key,)

# We create a prompt to reformulate the question
# based on the chat history that will be retrieved
# later on
contextualize_q_system_prompt = """Given a chat history and the latest user question \
which might reference context in the chat history, formulate a standalone question \
which can be understood without the chat history. Do NOT answer the question, \
just reformulate it if needed and otherwise return it as is."""
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

# We create a history-aware retriever.
# We leverage a helper function create_history_aware_retriever for this step, 
# which manages the case where chat_history is empty, 
# and otherwise applies prompt | model | StrOutputParser() | retriever in sequence.
history_aware_retriever = create_history_aware_retriever(
    model, 
    retriever, 
    contextualize_q_prompt
)

# We define the main prompt to ask the question
qa_system_prompt = """Answer the question based only on the following context.
Provide code examples when possible.
If you don't know the answer, you can appologize 
and tell the user that the question is out of your expertise.

The context is the following text: {context}
"""

qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

# Create a chain for passing a list of Documents to a model.
# Here we combine: 
# - The retrieved context
# - The conversation history
# - The query to generate an answer
question_answer_chain = create_stuff_documents_chain(
        model, 
        qa_prompt
    )

# We create a new chain using the history-aware retriever
# and the question_answer_chain
chain = create_retrieval_chain(
        history_aware_retriever, 
        question_answer_chain
    )

# We define a store to keep track of the chat history
# for each session
store = {}

# We define a function to get the chat history for a session
# based on the session ID
# If the session ID is not in the store, we create a new chat history
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


# RunnableWithMessageHistory is responsible for reading 
# and updating the chat message history.
# It takes the chain, the function to get the chat history,
# then injects the chat history and update it after each run
conversational_chain = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
    output_messages_key="answer",
)

# Create a loop to ask questions
while True:
    question = input("Enter your question: ")
    if question == "exit":
        break
    cc = conversational_chain.invoke(
            {
                "input": question,
            },
        config=
            {
                "configurable": 
                    {
                        "session_id": "1"
                    }
            },
    )["answer"]
    print(cc)
```


## No-Code Assistants


### ChatGPT's GPTs: Custom Plugins to Extend ChatGPT's Capabilities


### Dify: Open-Source Platform for Developing LLM Applications


```bash
git clone https://github.com/langgenius/dify.git

cd dify/docker
docker compose up -d
```


### FlowiseAI: Drag & Drop UI for LLM Flows Creation


```bash
git clone https://github.com/FlowiseAI/Flowise.git
```


```bash
cd Flowise/docker
cp .env.example .env
docker-compose up -d
```


### Langflow: A Dynamic Graph-Based Low-Code Platform for AI Applications


```bash
pip install langflow
```


```bash
langflow
```


### LLMStack: No-Code LLM Agents and Workflows


```bash
pip install llmstack
```


```bash
llmstack
```


## An Overview of AI Assistant Solutions


### Programming Approach


### Supported LLMs


### RAG Engine


### Workflow and Orchestration


### Agent-Based Approach


### Observability


### Enterprise Features: SSO/Access Control


### Local Deployment